# transcript.py
"""
Generates natural-sounding transcript segments from structured Markdown
using an LLM, incorporating image descriptions if available.
"""

import re
import os
import time
import requests
import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple

# Setup logger for this module
log = logging.getLogger(__name__)

# --- Helper Functions ---
# (load_image_descriptions, extract_image_info, call_llm_for_transcript remain unchanged)

def load_image_descriptions(image_desc_json_path: Optional[Path]) -> Dict[str, Dict]:
    """
    Load image descriptions from the JSON file generated by image_descriptor.py.

    Args:
        image_desc_json_path: Path to the image descriptions JSON file. Can be None.

    Returns:
        A dictionary mapping relative image source paths to their description dicts.
        Returns an empty dict if the path is None or file not found/invalid.
    """
    if not image_desc_json_path or not image_desc_json_path.is_file():
        if image_desc_json_path: # Only warn if a path was given but not found
             log.warning(f"Image descriptions JSON file not found: {image_desc_json_path}. Proceeding without image context.")
        else:
             log.info("No image description file provided. Proceeding without image context.")
        return {}

    try:
        log.info(f"Loading image descriptions from: {image_desc_json_path.name}")
        with open(image_desc_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Create a mapping of image paths to descriptions
        image_descriptions = {}
        # The JSON contains {md_filename: [list_of_image_dicts]}
        # We just need the list(s) of image dicts
        for md_filename, images_list in data.items():
            if not isinstance(images_list, list):
                log.warning(f"Unexpected format in image descriptions for {md_filename}. Expected list, got {type(images_list)}. Skipping.")
                continue
            for img_data in images_list:
                if not isinstance(img_data, dict):
                    log.warning(f"Unexpected item format in image list for {md_filename}. Expected dict, got {type(img_data)}. Skipping item.")
                    continue
                # Use the relative path (normalized) as the key
                img_path_rel = img_data.get('image_source_relative')
                if img_path_rel:
                    # Normalize path separators just in case
                    norm_img_path = img_path_rel.replace('\\', '/')
                    image_descriptions[norm_img_path] = {
                        'alt_text_generated': img_data.get('alt_text_generated', 'Alt text not available'),
                        'description_generated': img_data.get('description_generated', 'Description not available')
                    }
                else:
                     log.warning(f"Image data found in {md_filename} without 'image_source_relative'. Skipping item: {img_data.get('image_full_path')}")

        log.info(f"Loaded descriptions for {len(image_descriptions)} images.")
        return image_descriptions
    except json.JSONDecodeError:
        log.exception(f"Error decoding JSON from image descriptions file: {image_desc_json_path}")
        return {}
    except Exception as e:
        log.exception(f"Error loading image descriptions from {image_desc_json_path}: {e}")
        return {}


def extract_image_info(text: str, image_descriptions: Dict[str, Dict]) -> Tuple[str, List[Dict]]:
    """
    Finds Markdown image tags in text, replaces them with a placeholder,
    and returns the cleaned text and a list of extracted image info (including descriptions).

    Args:
        text: The input text chunk (potentially containing markdown images).
        image_descriptions: The loaded dictionary mapping relative image paths to descriptions.

    Returns:
        A tuple: (cleaned_text, image_info_list).
            cleaned_text: Text with image tags replaced by '[IMAGE LOCATION]'.
            image_info_list: List of dicts, each containing details about an image found.
    """
    image_info_list = []
    # Pattern to match markdown images: ![alt text](image path)
    image_pattern = re.compile(r'!\[(?P<alt>.*?)\]\((?P<src>.*?)\)')

    def replace_image_and_collect_info(match: re.Match) -> str:
        original_alt = match.group('alt')
        # Normalize path separators and strip whitespace
        img_path_rel = match.group('src').strip().replace('\\', '/')

        # Look up image descriptions using the normalized relative path
        desc_info = image_descriptions.get(img_path_rel, {})
        alt_generated = desc_info.get('alt_text_generated', f"Alt text unavailable for {img_path_rel}")
        desc_generated = desc_info.get('description_generated', f"Description unavailable for {img_path_rel}")

        log.debug(f"Found image: {img_path_rel}, Orig Alt: '{original_alt}', Gen Alt: '{alt_generated[:50]}...'")

        image_info = {
            'original_alt_text': original_alt,
            'image_path_relative': img_path_rel,
            'alt_text_generated': alt_generated,
            'description_generated': desc_generated
        }
        image_info_list.append(image_info)
        return '[IMAGE LOCATION]'  # Replace with placeholder

    # Substitute using the helper function
    cleaned_text = image_pattern.sub(replace_image_and_collect_info, text)
    # Strip potential leading/trailing whitespace resulting from substitution
    cleaned_text = cleaned_text.strip()

    if image_info_list:
        log.debug(f"Extracted {len(image_info_list)} image references from text chunk.")

    return cleaned_text, image_info_list


def call_llm_for_transcript(chunk_text: str, chunk_type_description: str, config: Dict[str, Any], image_info_list: Optional[List[Dict]] = None) -> Optional[str]:
    """
    Calls the configured LLM API to convert a text chunk into a natural-sounding transcript.

    Args:
        chunk_text: The text segment to convert.
        chunk_type_description: A description of the text segment's type (e.g., "Question Title").
        config: The pipeline configuration dictionary.
        image_info_list: Optional list of image details extracted from this chunk.

    Returns:
        The generated transcript string, or None on failure.
    """
    # --- Get config values ---
    model = config.get('LLM_TRANSCRIPT_MODEL_NAME')
    api_url = config.get('LLM_TRANSCRIPT_API_URL')
    api_key = config.get('LLM_TRANSCRIPT_API_KEY') # Optional for local Ollama
    timeout = config.get('TIMEOUTS', {}).get('llm_transcript', 240)
    # Temperature could also be configurable, using a default for now
    temperature = config.get('LLM_TRANSCRIPT_TEMPERATURE', 0.3)

    if not model or not api_url:
        log.error("LLM model name or API URL for transcript generation is not configured.")
        return None

    # --- Build Prompt ---
    prompt_parts = [f"""
You are an assistant that converts segments of an exam paper into natural-sounding transcript snippets for a Text-to-Speech (TTS) engine.

Input Text Type: {chunk_type_description}

Input Text:
\"\"\"
{chunk_text}
\"\"\"

Task:
Rewrite the 'Input Text' as a natural-sounding transcript segment.
If appropriate for this type (like starting a new section or question), add a brief introductory phrase (e.g., "Okay, next is...", "Now for...", "Moving on to...").
Keep the core meaning and all technical details of the original text absolutely intact.
Do NOT add explanations, summaries, or any content not present in the original text unless instructed to describe an image.
Focus on clear, speakable language.
Remove any markdown formatting like '##'.
Ensure numbers, technical terms, and variable names are pronounced clearly (e.g., write out "dollar t one" for $t1 if appropriate for speech, or keep as "$t1" if the TTS handles it well - default to keeping original).
"""]

    # Handle image information if available
    if image_info_list:
        prompt_parts.append("\n---\n\n")
        prompt_parts.append("**Image Handling Instructions:**\n")
        prompt_parts.append("If the input text contains '[IMAGE LOCATION]', replace it in the output transcript with the following spoken sequence for EACH image found in the original text chunk (using the details provided below):\n")
        prompt_parts.append("1. Say: 'There is a figure shown here.' (or similar phrase like 'A diagram is provided.')\n") # Adjusted phrasing slightly
        prompt_parts.append("2. Say: 'It illustrates: [concise alt text]'\n")
        prompt_parts.append("3. Say: 'Detailed description: [full image description]'\n\n")
        prompt_parts.append("Use the following image details for the placeholders:\n")

        for i, img_info in enumerate(image_info_list):
            prompt_parts.append(f"- Image {i + 1} Details:\n")
            prompt_parts.append(f"  - Concise Alt Text: {img_info['alt_text_generated']}\n")
            prompt_parts.append(f"  - Full Image Description: {img_info['description_generated']}\n")

    prompt_parts.append("\n---\n\nOutput ONLY the final spoken transcript text. Do not include headers, labels, or markdown code fences.\n")

    full_prompt = "".join(prompt_parts)
    log.debug(f"Transcript LLM Prompt (first 300 chars): {full_prompt[:300]}...")

    # --- Prepare Request ---
    headers = {"Content-Type": "application/json"}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"

    # --- Prepare Payload (Adapts for Ollama vs Chat APIs) ---
    if "ollama" in api_url or "localhost" in api_url: # Basic check for Ollama /generate
        payload = {
            "model": model,
            "prompt": full_prompt, # Ollama uses 'prompt'
            "stream": False,
            "options": {"temperature": temperature}
        }
    else: # Assume OpenAI/OpenRouter /chat/completions format
        payload = {
            "model": model,
            "messages": [ # Chat APIs use 'messages'
                {"role": "system", "content": "You generate natural-sounding transcript snippets for TTS from exam segments."}, # Simpler system prompt for chat
                {"role": "user", "content": full_prompt} # Pass the detailed instructions as user prompt
            ],
            "stream": False,
            "temperature": temperature # Common parameter name for chat APIs
        }

    # --- Make API Call ---
    try:
        log.info(f"Sending request to Transcript LLM (Model: {model})...")
        response = requests.post(api_url, headers=headers, json=payload, timeout=timeout)
        response.raise_for_status()
        data = response.json()
        log.debug(f"Transcript LLM Raw Response: {data}")

        # Extract content based on provider (Ollama vs OpenAI/OpenRouter)
        transcript = None
        if "response" in data: # Ollama /generate
            transcript = data.get("response", "").strip()
        elif "choices" in data and len(data["choices"]) > 0: # OpenAI/OpenRouter /chat/completions
            message = data["choices"][0].get("message", {})
            transcript = message.get("content", "").strip()

        if transcript:
             # Basic cleanup - remove potential leading/trailing quotes sometimes added by LLMs
            transcript = transcript.strip('"')
            log.info(f"Transcript segment received: '{transcript[:80]}...'")
            return transcript
        else:
             log.warning(f"LLM returned empty transcript content.")
             return None

    except requests.exceptions.Timeout:
        log.error(f"Request to Transcript LLM timed out after {timeout}s.")
        return None
    except requests.exceptions.HTTPError as e:
        log.error(f"HTTP error from Transcript LLM API ({api_url}): {e.response.status_code}")
        try: log.error(f"Response body: {e.response.text}")
        except Exception: pass
        return None
    except requests.exceptions.RequestException as e:
        log.exception(f"Error calling Transcript LLM API: {e}")
        return None
    except json.JSONDecodeError:
        log.error(f"Could not decode JSON response from Transcript LLM: {response.text}")
        return None
    except Exception as e:
        log.exception(f"Unexpected error during Transcript LLM call: {e}")
        return None


# --- Main Orchestrator Entry Point ---
def generate_transcripts_via_llm(input_md_file: Path, image_descriptions_file: Optional[Path], output_json_file: Path, config: Dict[str, Any]) -> bool:
    """
    Main function called by orchestrator. Generates transcripts from a markdown file,
    optionally using image descriptions, and saves the results as a JSON list.

    Args:
        input_md_file: Path to the formatted markdown input file.
        image_descriptions_file: Path to the JSON file with image descriptions (can be None).
        output_json_file: Path where the list of transcript dicts should be saved.
        config: The pipeline configuration dictionary.

    Returns:
        True if transcripts were generated and saved successfully, False otherwise.
    """
    start_time = time.time()
    log.info(f"Starting transcript generation for: {input_md_file.name}")
    all_transcripts = []  # List to store transcript dicts for JSON output
    doc_had_errors = False

    # Load image descriptions (handles None path internally)
    image_descriptions = load_image_descriptions(image_descriptions_file)

    # --- Read Input Markdown ---
    try:
        with open(input_md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        if not content.strip():
             log.warning(f"Input markdown file is empty: {input_md_file.name}. No transcripts to generate.")
             # Save empty list to output JSON? Or return False? Let's save empty list.
             output_json_file.parent.mkdir(parents=True, exist_ok=True)
             with open(output_json_file, 'w', encoding='utf-8') as jsonf:
                 json.dump([], jsonf)
             log.info("Saved empty transcript list.")
             return True
    except FileNotFoundError:
        log.error(f"Input markdown file not found: {input_md_file}")
        return False
    except Exception as e:
        log.exception(f"Error reading input markdown file {input_md_file}: {e}")
        return False

    # --- Split Markdown into Processable Sections ---
    # Split based on '## ' markdown H2 headings
    sections = re.split(r'\n(?=##\s)', content) # Ensure space after ##
    transcript_id_counter = 0 # Use simple counter for filename base

    # Regex for detecting subparts like (a), (b) etc. at the start of a line
    subpart_marker_pattern = re.compile(r"^\s*\(([a-zA-Z])\)", re.MULTILINE)

    model_name_for_log = config.get('LLM_TRANSCRIPT_MODEL_NAME', 'Unknown')
    log.info(f"Processing {len(sections)} potential sections using model: {model_name_for_log}")
    log.info("-" * 30)

    # --- Process Each Section ---
    for i, section_text in enumerate(sections):
        section_text = section_text.strip()
        if not section_text:
            continue

        log.debug(f"Processing section {i+1}...")
        # Extract H2 heading (if present) and body
        heading = ""
        body = section_text # Assume body is whole section initially
        if section_text.startswith("## "):
            parts = section_text.split('\n', 1)
            heading = parts[0].strip() # Includes the ##
            body = parts[1].strip() if len(parts) > 1 else ""

        # Determine Section Type (for logging/prompting)
        chunk_type = "General Section Content"
        heading_no_hash = heading.lstrip('#').strip() if heading else ""
        if heading:
            if i == 0: chunk_type = "Main Title or First Section"
            elif heading_no_hash.lower().startswith("instructions"): chunk_type = "Instructions Section"
            elif heading_no_hash.lower().startswith("appendix"): chunk_type = "Appendix Section"
            elif heading_no_hash.lower().startswith("part"): chunk_type = f"Start of {heading_no_hash}"
            elif re.match(r'\d+\.', heading_no_hash): chunk_type = f"Question Heading: {heading_no_hash}"
            else: chunk_type = f"Section Heading: {heading_no_hash}"
        elif i == 0:
            chunk_type = "Document Start (No Heading)"

        is_question_heading = heading and re.match(r'##\s*\d+\.', heading)

        # --- Process Question Sections with Subparts ---
        if is_question_heading:
            q_match = re.match(r'##\s*(\d+)\.(.*)', heading)
            q_num = q_match.group(1) if q_match else 'Unknown'
            q_title_text = q_match.group(2).strip() if q_match else heading_no_hash

            # <<< FIX: Use re.search and string slicing for intro/rest >>>
            first_subpart_match = subpart_marker_pattern.search(body)
            if first_subpart_match:
                intro_text_raw = body[:first_subpart_match.start()].strip()
                rest_of_body_raw = body[first_subpart_match.start():].strip()
            else:
                intro_text_raw = body # No subparts, intro is the whole body
                rest_of_body_raw = ""

            # A. Process Question Heading / Intro Text
            transcript_id_counter += 1
            intro_text_cleaned, intro_images = extract_image_info(intro_text_raw, image_descriptions)
            combined_intro_chunk = f"{heading}\n\n{intro_text_cleaned}".strip() # Combine heading + intro text
            intro_chunk_type = f"Question {q_num} Heading and Introduction" + (" (leading to subparts)" if rest_of_body_raw else " (no subparts)")
            intro_filename_base = f"{transcript_id_counter:03d}_q{q_num}_intro"

            log.info(f"Processing: {intro_filename_base} ({intro_chunk_type})")
            llm_output = call_llm_for_transcript(combined_intro_chunk, intro_chunk_type, config, intro_images)

            if llm_output:
                all_transcripts.append({
                    "id": transcript_id_counter,
                    "filename_base": intro_filename_base,
                    "chunk_type": intro_chunk_type,
                    "original_text_chunk": combined_intro_chunk,
                    "transcript": llm_output
                })
            else:
                log.error(f"Failed to generate transcript for {intro_filename_base}.")
                doc_had_errors = True
            time.sleep(0.5) # Small delay

            # B. Process Subparts (if any)
            if rest_of_body_raw:
                # <<< FIX: Use re.finditer to process subparts >>>
                subpart_matches = list(subpart_marker_pattern.finditer(rest_of_body_raw))
                for idx, current_match in enumerate(subpart_matches):
                    part_id = current_match.group(1)
                    # Content starts after the current marker's full match
                    start_pos = current_match.end()
                    # Content ends before the *next* marker starts, or at the end of the string
                    end_pos = subpart_matches[idx+1].start() if (idx + 1) < len(subpart_matches) else len(rest_of_body_raw)

                    part_content_raw = rest_of_body_raw[start_pos:end_pos].strip()
                    part_content_cleaned, part_images = extract_image_info(part_content_raw, image_descriptions)

                    if not part_content_cleaned and not part_images:
                         log.debug(f"Skipping empty subpart Q{q_num}({part_id}).")
                         continue # Skip empty subparts

                    transcript_id_counter += 1
                    subpart_chunk_type = f"Question {q_num}, Sub-part ({part_id})"
                    subpart_filename_base = f"{transcript_id_counter:03d}_q{q_num}_part_{part_id}"

                    log.info(f"Processing: {subpart_filename_base} ({subpart_chunk_type})")
                    # Add marker back for context in the LLM prompt? Helpful for LLM.
                    llm_input_chunk = f"({part_id}) {part_content_cleaned}"
                    llm_output = call_llm_for_transcript(llm_input_chunk, subpart_chunk_type, config, part_images)

                    if llm_output:
                          all_transcripts.append({
                               "id": transcript_id_counter,
                               "filename_base": subpart_filename_base,
                               "chunk_type": subpart_chunk_type,
                               "original_text_chunk": llm_input_chunk, # Store the text sent to LLM
                               "transcript": llm_output
                          })
                    else:
                          log.error(f"Failed to generate transcript for {subpart_filename_base}.")
                          doc_had_errors = True
                    time.sleep(0.5) # Small delay

        else:
            # --- Process General Sections / Non-Question Content ---
            # Use the *original* full section text here
            section_cleaned, section_images = extract_image_info(section_text, image_descriptions)

            if not section_cleaned and not section_images:
                log.debug(f"Skipping empty general section {i+1}.")
                continue # Skip empty sections

            transcript_id_counter += 1
            # Create a safer filename
            base_name_part = re.sub(r'[^a-z0-9_]+', '', chunk_type.lower().replace(' ', '_').replace(':', ''))[:20]
            general_filename_base = f"{transcript_id_counter:03d}_{base_name_part or 'general'}"

            log.info(f"Processing: {general_filename_base} ({chunk_type})")
            llm_output = call_llm_for_transcript(section_cleaned, chunk_type, config, section_images)

            if llm_output:
                all_transcripts.append({
                    "id": transcript_id_counter,
                    "filename_base": general_filename_base,
                    "chunk_type": chunk_type,
                    "original_text_chunk": section_cleaned,
                    "transcript": llm_output
                })
            else:
                log.error(f"Failed to generate transcript for {general_filename_base}.")
                doc_had_errors = True
            time.sleep(0.5) # Small delay

    # --- Save Final JSON ---
    log.info("-" * 30)
    if not all_transcripts:
        log.warning("No transcript segments were successfully generated.")
        # Decide if this is an error - returning True as the process ran.
    else:
         log.info(f"Generated {len(all_transcripts)} transcript segments.")

    try:
        output_json_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_json_file, 'w', encoding='utf-8') as jsonf:
            json.dump(all_transcripts, jsonf, indent=2, ensure_ascii=False) # Use indent=2 for readability
        elapsed = time.time() - start_time
        log.info(f"Transcript generation complete. Saved to: {output_json_file.name} in {elapsed:.2f}s.")
        # Return True even if some chunks failed, as long as the file was written
        return True
    except Exception as e:
        log.exception(f"Error writing final transcript JSON file: {output_json_file}")
        return False

# (No __main__ block needed)