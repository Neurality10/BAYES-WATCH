[
  {
    "id": 1,
    "filename_base": "001_main_title_or_first_",
    "chunk_type": "Main Title or First Section",
    "original_text_chunk": "## ELL788 Computational Perception & Cognition",
    "transcript": "Okay, next is ELL788 Computational Perception & Cognition."
  },
  {
    "id": 2,
    "filename_base": "002_instructions_section",
    "chunk_type": "Instructions Section",
    "original_text_chunk": "## Instructions\n- Please answer Part-A and Part-B in separate answer books.\n- Indicate the part clearly on top of the cover page of each answer book.",
    "transcript": "Okay, next is the instructions section. Please answer Part-A and Part-B in separate answer books. Indicate the part clearly on top of the cover page of each answer book."
  },
  {
    "id": 3,
    "filename_base": "003_start_of_part__a",
    "chunk_type": "Start of PART - A",
    "original_text_chunk": "## PART - A\n\n### 1. Short answer questions [4 x 1 = 4 marks]\n\n1. Assume that a person is driving a car, while listening to music. The car's driver assistance system detects a pedestrian ahead on the road and estimates that the driver needs to apply his brakes within 10-15 ms to avoid an accident. Which of the following alarms will be most effective in this case?\n    - Audio alarm, e.g. beeps from the speakers.\n    - Visual alarm, e.g. flashing light on the panel.\n    - Vibro-tactile alarm, e.g. a vibration on the seat.\nPlease justify your answer. Assume that the car has all the three provisions.\n\n2. Assume that you have to distinguish some regions with five gray-shade colors (as illustrated in the adjoining diagram in three shades, white [255], gray [127] and black [0]). What color values will you use for achieving best perceptual distinction? Please justify your answer. [Hint: log 2 256 = 8]\n\n3. a. Why does a haptic display require a high sampling rate?\n   b. What is the importance of Surface Contact Point (SCP)?\n\n4. a. Explain the principles behind MP3 and AAC perceptual audio coding schemes.\n   b. What are the main differences between coding schemes used for music system and for telephony / conferencing?\n\n5. a. What are the factors responsible for perception of size in human vision?\n   b. Why does the left arrow in the adjoining figure generally appear to be shorter than the right one?\n\n[IMAGE LOCATION]\n\n[IMAGE LOCATION]",
    "transcript": "Okay, next is PART - A.\n\nNow for Short answer questions. The instructor asks you to assume that a person is driving a car while listening to music. The driver assistance system detects a pedestrian ahead on the road and estimates that the driver needs to apply their brakes within 10-15 milliseconds to avoid an accident. Which of the following alarms will be most effective in this case?\n\nYou have three options: audio alarm, e.g., beeps from the speakers; visual alarm, e.g., flashing light on the panel; or vibro-tactile alarm, e.g., a vibration on the seat. Please choose one and justify your answer. Assume that the car has all three provisions.\n\nAssuming you have to distinguish some regions with five gray-shade colors as illustrated in the adjoining diagram in three shades, white [255], gray [127], and black [0]. What color values will you use for achieving best perceptual distinction? Please justify your answer. Hint: log base 2 of 256 equals 8.\n\nA haptic display requires a high sampling rate. Can you explain why?\n\nThe importance of Surface Contact Point, or SCP, is also crucial in this context.\n\nMoving on to audio coding schemes. Explain the principles behind MP3 and AAC perceptual audio coding schemes.\n\nNow let's compare these schemes used for music systems with those used for telephony / conferencing. What are the main differences between them?\n\nA person perceives size in human vision based on several factors. Can you explain what they are? And why does the left arrow in the adjoining figure generally appear to be shorter than the right one?\n\nThere is a figure shown here. It illustrates: Error processing image: images/ell788_maj-img-1.png\n\nDetailed description of this image is not available.\n\nThere is a figure shown here. It illustrates: Error processing image: images/ell788_maj-img-2.png\n\nDetailed description of this image is not available."
  },
  {
    "id": 4,
    "filename_base": "004_start_of_part__b",
    "chunk_type": "Start of PART - B",
    "original_text_chunk": "## PART - B\n\n### 6. Long answer questions [5 x 3 = 15 marks]\n\n6. Describe the key features of the massive modularity hypothesis of Cosmides and Tooby. How did this hypothesis come about on the basis of human performance on the Wason selection test? What do you think are the key weaknesses of this hypothesis? Is it compatible with Fodorean modularity: why or why not?\n\n### 7. Long answer questions [5 x 3 = 15 marks]\n\n7. In what way does ACT-R incorporate Bayesian inference? Explain using an example.\n\n### 8. Long answer questions [5 x 3 = 15 marks]\n\n8. In this course we have discussed two major types of cognitive modelling approaches: Bayesian and connectionist (neural network). In this question we will try to explore a simple example that can combine the two approaches. Consider a very basic neural network with just two inputs, x1 and x2, and a single neuron which computes y = σ(w0 + w1x1 + w2x2), where σ is the logistic sigmoid σ(a) = 1/(1 + e^-a). In other words, we are just referring to a standard logistic regression model. Further suppose that we have a data set with N points D = {(x11, x12, t1); (x21, x22, t2); ... (xN1, xN2, tN)}, where ti ∈ {0, 1} is the true output for data point i. As usual in logistic regression, we will interpret the model output y as the probability that the actual output is 1, i.e., p(ti = 1|x1i, x2i) = y(x1i, x2i).\n\n   a. Draw the neural network, labeling all nodes and links.\n   b. Normally neural networks are trained via backpropagation. Variants of this are also used in all the deep learning approaches we talked about. But here, we would like to use a Bayesian approach to infer the weights w = (w0, w1, w2). Write down Bayes' rule for the posterior distribution on the weights given the data, i.e., p(w|D).\n   c. One term on the right hand side is the likelihood of the data. Given the model and data as described above, can you expand this term to explicitly write down the likelihood as a function of the model parameters and individual data points?\n   d. The other term dependent on w is the prior. Can you give some reasonable choice for this?\n   e. Having done parts (c) and (d), you get the posterior as a function of the given data points and the parameters. One can now maximise this (if not analytically then numerically or heuristically) to get the MAP estimates for the parameters w. How do you think these estimates relate to those that would have been obtained using standard backpropagation? Can you think of any advantage the Bayesian approach might have, especially in the context of the kinds of cognitive modelling tasks we've looked at?\n\n### 9. Long answer questions [5 x 3 = 15 marks]\n\n9. Consider the following three sentences:\n   1. The daughter of the colonel who had a black dress left the party.\n   2. The daughter of the colonel who had a black mustache left the party.\n   3. The brother of the colonel who had a black mustache left the party.\n   \n   a. According to the race-based model of sentence processing, which of these three should be easiest to process? Explain why.\n   b. The easiest to process sentence is actually problematic: can you say why? Does this indicate a flaw with race-based sentence processing?",
    "transcript": "Okay, next is PART B.\n\nSo, let's move on to long answer questions worth 15 marks each. First up, we have question six. Can you describe the key features of Cosmides and Tooby's massive modularity hypothesis? How did this idea come about based on human performance in the Wason selection test? What are some potential weaknesses of this hypothesis? And how does it relate to Fodorean modularity?\n\nMoving on to question seven, can you explain how ACT-R incorporates Bayesian inference using an example?\n\nNow, let's dive into question eight. We've discussed two main types of cognitive modelling approaches: Bayesian and connectionist models. In this question, we're going to explore a simple example that combines both approaches. Consider a basic neural network with just two inputs, x1 and x2, and a single neuron computing y = σ(w0 + w1x1 + w2x2). We'll use a standard logistic regression model here. Suppose we have a dataset with N points D = {(x11, x12, t1); (x21, x22, t2); ... (xN1, xN2, tN)}, where ti ∈ {0, 1} is the true output for data point i.\n\nFirst, can you draw the neural network, labeling all nodes and links? Next, normally neural networks are trained via backpropagation. However, we'd like to use a Bayesian approach to infer the weights w = (w0, w1, w2) given the data. Can you write down Bayes' rule for the posterior distribution on the weights?\n\nNow, let's expand the term on the right-hand side of this equation - can you explicitly write down the likelihood as a function of the model parameters and individual data points? What prior do you think would be reasonable to use here? Once we have those, we get the posterior as a function of the given data points and parameters. We can then maximise this to get the MAP estimates for the parameters w.\n\nMoving on to question nine, consider these three sentences:\n1. The daughter of the colonel who had a black dress left the party.\n2. The daughter of the colonel who had a black mustache left the party.\n3. The brother of the colonel who had a black mustache left the party.\n\nCan you explain according to the race-based model of sentence processing, which of these three should be easiest to process? And can you say why this is problematic - does it indicate a flaw with race-based sentence processing?"
  }
]